---
title: "02_training_model"
author: "Miguel √Ångel Armengol de la Hoz"
date: "11/5/2019"
output:
  html_notebook:
    theme: united
    toc: yes
---

# Environment

```{r}
require(foreign)
require(ggplot2)
require(MASS)
require(Hmisc)
require(reshape2)
require(caret)
require(boot)
require(pROC)
library(mlbench)
library(MLmetrics)
library(plotly)
library(gbm)
library(ggthemr)
library(xgboost)
library(oddsratio)
```

# Random Hyperparameter Tunning

The default method for optimizing tuning parameters in train is to use a grid search. This approach is usually effective but, in cases when there are many tuning parameters, it can be inefficient. An alternative is to use a combination of grid search and racing. Another is to use a random selection of tuning parameter combinations to cover the parameter space to a lesser extent.

Using [caret](https://topepo.github.io/caret/).

_We can adress later the tuning parameters approach_

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           search = "random")
```

## Random selection of tuning parameter combinations

Here we are first adressing several Machine Learning methods.
There are more methods that can be addressed [Available Models in caret::train](https://rdrr.io/cran/caret/man/models.html)

```{r}
# Machine learning methods
gbmFit <- train( actualhospitalmortality ~ 
                + apachescore
                + final_charlson_score
                + q_leaking_index
                ,data = train,
                method = "gbm",
                trControl = fitControl,
                verbose = FALSE,
                metric = "ROC" ## Specify which metric to optimize
)

svmFit <- train( actualhospitalmortality ~
                + apachescore
                + final_charlson_score
                + q_leaking_index
                ,data = train,
                method = "svmRadial",
                trControl = fitControl,
                preProc = c("center", "scale"),
                tuneLength = 8,
                metric = "ROC" ## Specify which metric to optimize
)

rfFit <- train( actualhospitalmortality ~
                + apachescore
                + final_charlson_score
                + q_leaking_index
                ,data = train,
                method = "rf",
                trControl = fitControl,
                verbose = FALSE,
                metric = "ROC" ## Specify which metric to optimize
)

xgbFit <- train( actualhospitalmortality ~
                + apachescore
                + final_charlson_score
                + q_leaking_index
                ,data = train,
                method = "xgbTree",
                trControl = fitControl,
                verbose = FALSE,
                metric = "ROC" ## Specify which metric to optimize
)

nnFit <- train( actualhospitalmortality ~
                + apachescore
                + final_charlson_score
                + q_leaking_index
                ,data = train,
                method = "nnet",
                trControl = fitControl,
                verbose = FALSE,
                metric = "ROC" ## Specify which metric to optimize
)


lrFit <- train( actualhospitalmortality ~
                + apachescore
                + final_charlson_score
                + q_leaking_index
                ,data = train,
                method = "LogitBoost",
                trControl = fitControl,
                verbose = FALSE,
                metric = "ROC" ## Specify which metric to optimize
)

gamFit <- train( actualhospitalmortality ~ 
                + apachescore
                + final_charlson_score
                + q_leaking_index
                ,data = train,
                method = "gam",
                trControl = fitControl,
                verbose = T,
                metric = "ROC" ## Specify which metric to optimize
)
```

## Best models comprarision

```{r}
resamps <- resamples(list( gbmFit = gbmFit
                          ,svmFit = svmFit
                          ,rfFit  = rfFit
                          ,xgbFit = xgbFit
                          ,nnFit = nnFit
                          ,lrFit = lrFit
                          ,gamFit = gamFit
                          ))
summary_resamps<-summary(resamps)

summary_resamps<-as.data.frame(summary_resamps$statistics)
summary_resamps
```


# Selecting the model with the best performance

```{r}
# we save the best performing model (based on its ROC) and its name
best_performing_model<-get(
  rownames(summary_resamps[which(summary_resamps$ROC.Median==max(summary_resamps$ROC.Median))]
)
)
#manually select it
best_performing_model<-gamFit
best_performing_model_name<-best_performing_model$method # extracts name as string from model
```

We can see **`r best_performing_model_name`** is the model with the best performance, with a Median AUROC of **`r max(summary_resamps$ROC.Median)`**.  

Its best Random Hyperparameter Tune was:  
`r best_performing_model$bestTune`

# Evaluating the predictor on our test dataset

## Creating prediction-probabilities dataset

```{r}
prediction_probabilities<-predict(best_performing_model, newdata = test,type = "prob") # We create the probabilities dataset using our best performing model.

final_predictions<-cbind(test_Y,prediction_probabilities) # we bind our prediction with the actual data
final_predictions<-rename(final_predictions, obs = test_Y) # the function twoClassSummary reads the actual outcome as 'obs'
final_predictions['pred']<-ifelse(final_predictions$ALIVE > .83 # we have set the threshold in .5 this can be optimized until best performance is achieved
                                  , 'ALIVE','EXPIRED'
)

# Setting proper data types
final_predictions$obs<-as.factor(final_predictions$obs)
final_predictions$pred<-as.factor(final_predictions$pred)
```

## Geting evaluation insights

```{r}
insights_1<-as.data.frame(twoClassSummary(final_predictions, lev = levels(final_predictions$obs)))
names(insights_1)<-best_performing_model_name
insights_1<-t(insights_1) # we traspose it for better merging it.

insights_2<-as.data.frame(prSummary(final_predictions, lev = levels(final_predictions$obs)))
names(insights_2)<-best_performing_model_name
insights_2<-t(insights_2) # we traspose it for better merging it.

evaluation_insights<-cbind(insights_1,insights_2)
evaluation_insights<-as.data.frame(evaluation_insights)
evaluation_insights<-round(evaluation_insights,2)
evaluation_insights$Recall <-NULL # We already have specificity which is = recall
evaluation_insights$AUC<-NULL # We select the ROC from the first package so we remove this parameter
#renaming metric
evaluation_insights<-evaluation_insights%>%rename(AUROC = ROC) 

# we traspose the data for its representation
evaluation_insights<-t(evaluation_insights)
evaluation_insights<-as.data.frame(evaluation_insights)
evaluation_insights['Metrics']<-rownames(evaluation_insights)

# how to order the bars
evaluation_insights$Insights <- factor(evaluation_insights$Metrics
                     , levels = unique(evaluation_insights$Insights)[order(evaluation_insights$Metrics, decreasing = T)])

p <- plot_ly(
  data = evaluation_insights,
  x = evaluation_insights[,1],
  y = ~Metrics,
  text = evaluation_insights[,1],
  textposition='auto',
  type = "bar") %>%
  layout( title = paste(best_performing_model_name,"Model Metrics"))

p
```

# ROC Plot
```{r}
library(ROCR)
df <- cbind(list(1- final_predictions$ALIVE), list(as.numeric(final_predictions$obs) - 1))
pred <- prediction(df[1], df[2])
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
```


# Variables Importance

```{r message=TRUE, warning=TRUE}
ggthemr('flat')
varimp<-ggplot(varImp(best_performing_model, scale = T))+theme_minimal() 
ggplotly(varimp)
```

# Odds ratio 

Valid for gam logreg and glm.

We are using the or_gam function

```{r}
# Odds Ratio Calculations

train_for_or_calculation<-train
# or_gam only deals with numerical vars
train_for_or_calculation$actualhospitalmortality[train_for_or_calculation$actualhospitalmortality=='ALIVE']<-0 
train_for_or_calculation$actualhospitalmortality[train_for_or_calculation$actualhospitalmortality=='EXPIRED']<-1
train_for_or_calculation$actualhospitalmortality<-as.numeric(train_for_or_calculation$actualhospitalmortality)

# We need to run the gam separately (the function I am using is not compatible with th gam object caret creates, so I am training a new model just using gam().

gam_for_or_calculation<-gam(actualhospitalmortality ~ 
                + apachescore
                + final_charlson_score
                + q_leaking_index
                ,data = train_for_or_calculation)

# every exposure needs to be addressed separately the min value as a reference.
# every factor in the categorical exposures need to be addressed separately taking into account the min value as a reference.

# q_leaking_index Ors
or_gam(data = train_for_or_calculation, model = gam_for_or_calculation, pred = "q_leaking_index",values = c("1", "2"))
or_gam(data = train_for_or_calculation, model = gam_for_or_calculation, pred = "q_leaking_index",values = c("1", "3"))
or_gam(data = train_for_or_calculation, model = gam_for_or_calculation, pred = "q_leaking_index",values = c("1", "4"))

# rest of the exposures

or_gam(data = train_for_or_calculation, model = gam_for_or_calculation, pred = "apachescore",values = c(min(train_for_or_calculation$apachescore), max(train_for_or_calculation$apachescore)))
or_gam(data = train_for_or_calculation, model = gam_for_or_calculation, pred = "final_charlson_score",values = c(min(train_for_or_calculation$final_charlson_score), max(train_for_or_calculation$final_charlson_score)))
```




